{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4CABM2kA9LUKXn2cXonC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoYongJIn/YongJin-Repository/blob/main/%08PyTorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 가능 여부 확인\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 텐서 정의\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# 텐서를 GPU로 이동\n",
        "x = x.to(device)\n",
        "\n",
        "print(x.device) # 결과를 보면 GPU사용 가능 환경인지 알수있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKKgJ_jlWE_f",
        "outputId": "c40351d8-72d9-4888-bfd3-11bc8716d835"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "외부 GPU(eGPU) 사용: Thunderbolt 3 포트를 통해 외부 GPU를 맥북에 연결할 수 있습니다.\n",
        "이 방법은 추가적인 하드웨어 구매가 필요하며, 모든 맥 모델과 호환되지 않을 수 있습니다.\n",
        "'''"
      ],
      "metadata": {
        "id": "_F6GNCqgXd9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.rand(2,3))\n",
        "print(torch.randint(low=0, high=8, size=(2,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yADJAupIVXHM",
        "outputId": "37057910-f60e-45a9-e543-a7309c58c9b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8327, 0.9465, 0.6365],\n",
            "        [0.6430, 0.2195, 0.0284]])\n",
            "tensor([[3, 3, 7],\n",
            "        [7, 5, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_zeros = torch.zeros_like(x.cuda()) # Error: Found no NVIDIA driver on your system. 때문에 cuda기반 연산이 안된다."
      ],
      "metadata": {
        "id": "UFazWhUTWE81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 사용할 디바이스 설정\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# 텐서 x 정의 및 디바이스에 할당\n",
        "x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "\n",
        "# x와 같은 크기의 0으로 채워진 텐서 생성\n",
        "x_zeros = torch.zeros_like(x)\n",
        "\n",
        "print(x_zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3VTYPZ-aR6M",
        "outputId": "03d04db4-d911-4ea6-f561-3baafb8c9b33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "tensor([0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = torch.FloatTensor(np.array([[1,2,3],[4,5,6]]))\n",
        "print(a.type)\n",
        "print(a)\n",
        "\n",
        "b = torch.LongTensor(np.array([[1,2,3],[4,5,6]]))\n",
        "print(b.type)\n",
        "print(b)\n",
        "\n",
        "c = torch.ByteTensor(np.array([[1,2,3],[4,5,6]]))\n",
        "print(c.type)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHQrTArebMoh",
        "outputId": "6e015acd-2f93-41bc-d802-54198c419125"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method type of Tensor object at 0x79578d4918f0>\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "<built-in method type of Tensor object at 0x79578d5df100>\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "<built-in method type of Tensor object at 0x79578d659210>\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]], dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 24개 요소를 가진 1차원 텐서 생성\n",
        "x = torch.arange(24)\n",
        "\n",
        "# x를 2x2x6 차원으로 재구성\n",
        "print(x.view(2, 2, 6))\n",
        "\n",
        "print(x.view(-1,1,12))\n",
        "'''\n",
        "-1은 PyTorch가 해당 차원의 크기를 자동으로 계산하도록 지시합니다. 이는 \"나머지 모든 요소들을 포함하는 차원 크기를 자동으로 계산해 달라\"는 의미입니다.\n",
        "다시 말해, 텐서의 전체 요소 수를 유지하면서 지정된 다른 차원의 크기에 맞춰 자동으로 크기를 조정합니다.\n",
        "'''"
      ],
      "metadata": {
        "id": "XEKOToeMbMq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 24개 요소를 가진 1차원 텐서 생성\n",
        "x = torch.arange(24)\n",
        "\n",
        "# x를 2x2x6 차원으로 재구성\n",
        "print(x.view(2, 2, 6))\n",
        "\n",
        "x_transposed = x.transpose(0,1) # 텐서의 첫번째 차원과 두번째 차원이 바뀌는것으로 두차원이 없이면 오류가된다.\n",
        "# view vs reshape"
      ],
      "metadata": {
        "id": "W-dgaIrybMte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 24개 요소를 가진 1차원 텐서 생성\n",
        "x = torch.arange(24)\n",
        "\n",
        "# x를 2x2x6 차원으로 재구성\n",
        "x_reshaped = x.view(2, 2, 6)\n",
        "print(x_reshaped)\n",
        "\n",
        "# x_reshaped를 전치\n",
        "x_transposed = x_reshaped.transpose(0, 1)\n",
        "print(x_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xMCiap5rt6p",
        "outputId": "a5b29f87-f0e5-4fb5-c844-22d6cafefe69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
            "         [ 6,  7,  8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15, 16, 17],\n",
            "         [18, 19, 20, 21, 22, 23]]])\n",
            "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
            "         [12, 13, 14, 15, 16, 17]],\n",
            "\n",
            "        [[ 6,  7,  8,  9, 10, 11],\n",
            "         [18, 19, 20, 21, 22, 23]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 크기가 1인 차원을 포함하는 텐서 생성\n",
        "x = torch.zeros(2, 1, 2)  # 크기: 2x1x2(행방향으로 요소두개즉 열의개수이고, 열방향으로 요소 한개 즉 행의개수인것이다.)\n",
        "\n",
        "print(\"Original tensor:\")\n",
        "print(x)\n",
        "print(\"Shape of the original tensor:\", x.shape)\n",
        "\n",
        "# squeeze()를 사용하여 크기가 1인 차원 제거\n",
        "y = x.squeeze()\n",
        "\n",
        "print(\"\\nTensor after squeeze:\")\n",
        "print(y)\n",
        "print(\"Shape of the tensor after squeeze:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mERI6F9CG8u",
        "outputId": "8bd11384-c0a2-48f7-9b47-663841f0a880"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[[0., 0.]],\n",
            "\n",
            "        [[0., 0.]]])\n",
            "Shape of the original tensor: torch.Size([2, 1, 2])\n",
            "\n",
            "Tensor after squeeze:\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Shape of the tensor after squeeze: torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 두 텐서 생성\n",
        "a = torch.tensor([1, 2, 3, 4, 5])\n",
        "b = torch.tensor([3, 3, 3, 3, 3])\n",
        "\n",
        "# 같음 (Equal)\n",
        "eq = torch.eq(a, b) # a == b\n",
        "print(\"Equal (a == b):\", eq)\n",
        "\n",
        "# 크거나 같음 (Greater than or equal)\n",
        "ge = torch.ge(a, b) # a >= b\n",
        "print(\"Greater than or equal (a >= b):\", ge)\n",
        "\n",
        "# 큼 (Greater than)\n",
        "gt = torch.gt(a, b) # a > b\n",
        "print(\"Greater than (a > b):\", gt)\n",
        "\n",
        "# 작거나 같음 (Less than or equal)\n",
        "le = torch.le(a, b) # a <= b\n",
        "print(\"Less than or equal (a <= b):\", le)\n",
        "\n",
        "# 작음 (Less than)\n",
        "lt = torch.lt(a, b) # a < b\n",
        "print(\"Less than (a < b):\", lt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XCDHEX1CG-y",
        "outputId": "f0efdc40-d1aa-49b4-8cce-4960106c043f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equal (a == b): tensor([False, False,  True, False, False])\n",
            "Greater than or equal (a >= b): tensor([False, False,  True,  True,  True])\n",
            "Greater than (a > b): tensor([False, False, False,  True,  True])\n",
            "Less than or equal (a <= b): tensor([ True,  True,  True, False, False])\n",
            "Less than (a < b): tensor([ True,  True, False, False, False])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# STEP 함수 정의\n",
        "def STEP(x):\n",
        "    return x > 0\n",
        "\n",
        "# AND 게이트 함수 정의\n",
        "def AND(x, w, b):\n",
        "    z = torch.dot(x, w) + b\n",
        "    a = STEP(z)\n",
        "    return a\n",
        "\n",
        "# NAND 게이트 함수 정의\n",
        "def NAND(x, w, b):\n",
        "    z = torch.dot(x, w) + b\n",
        "    a = STEP(z)\n",
        "    return a\n",
        "\n",
        "# OR 게이트 함수 정의\n",
        "def OR(x, w, b):\n",
        "    z = torch.dot(x, w) + b\n",
        "    a = STEP(z)\n",
        "    return a\n",
        "\n",
        "# 각 게이트에 알맞은 w, b 가중치 생성\n",
        "w_and = torch.Tensor([0.5, 0.5])\n",
        "b_and = torch.Tensor([-0.7])\n",
        "w_nand = torch.Tensor([-0.5, -0.5])\n",
        "b_nand = torch.Tensor([0.7])\n",
        "w_or = torch.Tensor([0.5, 0.5])  # OR 게이트용 가중치 수정\n",
        "b_or = torch.Tensor([-0.2])      # OR 게이트용 바이어스 수정\n",
        "\n",
        "# 입력 텐서를 만들어 결과 확인\n",
        "X = torch.Tensor([[0,0], [0,1], [1,0], [1,1]])\n",
        "for x in X:\n",
        "    print(\"input :\", x.long().tolist())\n",
        "    print(\"-AND:\", AND(x, w=w_and, b=b_and).item())\n",
        "    print(\"-NAND:\", NAND(x, w=w_nand, b=b_nand).item())\n",
        "    print(\"-OR:\", OR(x, w=w_or, b=b_or).item())\n",
        "    print(\"--\" * 15)\n",
        "\n",
        "'''\n",
        ".item()은 PyTorch에서 사용되는 함수로, 주로 단일 요소를 가진 텐서에서 Python의 표준 데이터 타입(예: int, float)으로 값을 추출할 때 사용됩니다.\n",
        "이 메소드는 하나의 요소를 가진 텐서에서 그 값만을 꺼내고 싶을 때 유용하며, 텐서가 단 하나의 요소만을 포함할 때만 사용할 수 있습니다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "3BehHX4sCHBG",
        "outputId": "995842a6-e0f6-45ce-f796-9d91b972733f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input : [0, 0]\n",
            "-AND: False\n",
            "-NAND: True\n",
            "-OR: False\n",
            "------------------------------\n",
            "input : [0, 1]\n",
            "-AND: False\n",
            "-NAND: True\n",
            "-OR: True\n",
            "------------------------------\n",
            "input : [1, 0]\n",
            "-AND: False\n",
            "-NAND: True\n",
            "-OR: True\n",
            "------------------------------\n",
            "input : [1, 1]\n",
            "-AND: True\n",
            "-NAND: False\n",
            "-OR: True\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n.item()은 PyTorch에서 사용되는 함수로, 주로 단일 요소를 가진 텐서에서 Python의 표준 데이터 타입(예: int, float)으로 값을 추출할 때 사용됩니다.\\n이 메소드는 하나의 요소를 가진 텐서에서 그 값만을 꺼내고 싶을 때 유용하며, 텐서가 단 하나의 요소만을 포함할 때만 사용할 수 있습니다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR문제 해결을 위한 다중퍼셉트론 개발\n",
        "def solution(x, **kwargs):\n",
        "    # 새로운 층\n",
        "    s1 = NAND(x, w=kwargs['w_nand'], b=kwargs['b_nand'])\n",
        "    s2 = OR(x, w=kwargs['w_or'], b=kwargs['b_or'])\n",
        "    # 출력층: s1과 s2를 새로운 입력으로 구성하여 전달\n",
        "    y = AND(torch.Tensor([s1, s2]), w=kwargs['w_and'], b=kwargs['b_and'])\n",
        "    return y\n",
        "\n",
        "# 입력 텐서와 타겟 텐서를 생성\n",
        "input_data = torch.FloatTensor([[0,0], [1,0], [0,1], [1,1]])\n",
        "target_data = torch.ByteTensor([0, 1, 1, 0])\n",
        "\n",
        "# 이전에 사용한 가중치를 dictionary 타입으로 만든다.\n",
        "kwargs = dict([('w_nand', w_nand), ('b_nand', b_nand), ('w_or', w_or), ('b_or', b_or), ('w_and', w_and), ('b_and', b_and)])\n",
        "\n",
        "# 테스트\n",
        "for x, y in zip(input_data, target_data):\n",
        "    pred = solution(x, **kwargs)\n",
        "    print(\"Predict:\", pred.item(), \"| Target:\", y.item())\n",
        "\n",
        "'''\n",
        "**kwargs는 Python 프로그래밍 언어에서 함수를 정의할 때 사용되는 문법으로,\n",
        "키워드 인자(keyword arguments)의 가변적인 수를 받을 수 있게 해줍니다. **kwargs는 \"keyword arguments\"의 약어로,\n",
        "함수에 전달되는 키워드 인자들을 딕셔너리 형태로 받습니다.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Python의 zip 함수는 여러 개의 이터러블(iterable, 예: 리스트, 튜플 등)을 인자로 받아,\n",
        "각 이터러블에서 동일한 인덱스에 있는 요소들을 묶어서 이터레이터(iterator)를 생성합니다.\n",
        "간단히 말해, 여러 개의 시퀀스를 병렬적으로 순회할 수 있게 해주는 함수입니다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "RA_I99wtSChk",
        "outputId": "00f9bc29-0c72-48e7-b60d-425dfae2f4ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict: False | Target: 0\n",
            "Predict: True | Target: 1\n",
            "Predict: True | Target: 1\n",
            "Predict: False | Target: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n**kwargs는 Python 프로그래밍 언어에서 함수를 정의할 때 사용되는 문법으로,\\n키워드 인자(keyword arguments)의 가변적인 수를 받을 수 있게 해줍니다. **kwargs는 \"keyword arguments\"의 약어로,\\n함수에 전달되는 키워드 인자들을 딕셔너리 형태로 받습니다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기계학습에서 사용되는 신경망 구조는 인간의 뇌 구조에서 영감을 받았지만, 실제 생물학적 뇌 구조와는 상당히 다릅니다. 초기 인공신경망은 뇌의 신경세포(뉴런)의 연결 방식을 모방하여 설계되었으나, 이는 매우 단순화된 형태였습니다.\n",
        "\n",
        "주요 차이점:\n",
        "구조적 단순화: 인공신경망의 뉴런은 생물학적 뉴런에 비해 매우 단순합니다. 인공 뉴런은 주로 입력, 가중치, 활성화 함수, 출력으로 구성되어 있으며, 복잡한 생물학적 과정을 모방하지 않습니다.\n",
        "\n",
        "학습 메커니즘: 인공신경망은 주로 역전파(Backpropagation) 알고리즘과 경사 하강법(Gradient Descent)을 사용하여 학습합니다. 이는 뇌의 학습 방식과는 다른, 수학적으로 정의된 방법입니다.\n",
        "\n",
        "처리 방식: 인공신경망은 주로 명확하게 정의된 작업(예: 이미지 분류, 텍스트 번역)에 최적화되어 있습니다. 반면, 인간의 뇌는 다양한 유형의 정보를 동시에 처리하고, 유연하게 새로운 상황에 적응할 수 있습니다.\n",
        "\n",
        "연결 복잡도: 인간 뇌의 연결은 인공신경망보다 훨씬 복잡하고 동적입니다. 뇌는 수십억 개의 뉴런과 수조 개의 시냅스 연결을 가지고 있으며, 이러한 연결은 지속적으로 변화합니다.\n",
        "\n",
        "결론:\n",
        "인공신경망은 생물학적 뇌의 기본적인 아이디어를 차용했지만, 그 설계와 작동 방식은 실제 뇌의 복잡성과 유연성을 완전히 반영하지는 못합니다. 인공신경망의 개발은 주로 수학적, 공학적 접근을 기반으로 하며, 특정한 종류의 계산 문제를 효율적으로 해결하는 데 중점을 둡니다. 따라서, 이러한 네트워크는 생물학적 뇌를 이해하는 데 직접적으로 사용되기보다는, 뇌에서 영감을 받은 별개의 컴퓨팅 모델로 간주됩니다.\n",
        "\n",
        "그러나 발전함에따라 거의 유사해지고 있는것같습니다."
      ],
      "metadata": {
        "id": "_4IyeX2cbWNe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4mPuvRUSCj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한마디로 정리하자면, 파이토치(PyTorch)는 심층 신경망을 구축하고 훈련시키기 위해 주로 사용되는 라이브러리이며, 이 과정에서 텐서 연산을 효율적으로 수행하기 위해 GPU를 활용할 수 있습니다. 파이토치는 GPU를 이용한 고속 텐서 연산 지원 외에도 자동 미분, 다양한 신경망 구조 및 훈련 알고리즘을 제공하여 딥러닝 연구 및 개발에 널리 사용됩니다."
      ],
      "metadata": {
        "id": "xMldvscHaHVx"
      }
    }
  ]
}