{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ab55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "softmax지나면 확률값나온다 여러번 시행해서 가장 많이 나온것을 택하거나 확률이 가장높은것을\n",
    "택하거나 확률이 보이게 해보자 softmax로 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a57888a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def generate_data(num_trials):\n",
    "    trials = []\n",
    "    for _ in range(num_trials):\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            if random.randint(1, 125) == 1:\n",
    "                break\n",
    "        trials.append(count)\n",
    "    return trials\n",
    "\n",
    "x_data = generate_data(1000)\n",
    "\n",
    "#print(\"X 데이터:\", x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b9ee7049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_numbers_based_on_range(numbers, ranges):\n",
    "    new_list = []\n",
    "    for number in numbers:\n",
    "        assigned = False\n",
    "        for range_start, range_end, assigned_number in ranges:\n",
    "            if range_start <= number <= range_end:\n",
    "                new_list.append(assigned_number)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            new_list.append(ranges[-1][2])  # 범위에 속하지 않는 경우 마지막 숫자를 할당(range의 마지막 요소 세번째 요소)\n",
    "    return new_list\n",
    "\n",
    "# 사용 예시\n",
    "numbers = x_data\n",
    "ranges = [(1, 51, 1), (51, 101, 2), (101, 151, 3), (151, 201, 4),(201, 251, 5),(251, 301, 6)] # 각 세번째 요소는 할당할 숫자를 의미합니다.\n",
    "normalization_x_data = assign_numbers_based_on_range(numbers, ranges)\n",
    "#print(\"normalization_x_data =\", normalization_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7825c408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (697, 3), Validation Set: (150, 3), Test Set: (150, 3)\n",
      "[[4 6 2]\n",
      " [6 2 2]\n",
      " [2 2 1]\n",
      " ...\n",
      " [4 2 1]\n",
      " [2 1 1]\n",
      " [1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 예시 (리스트 형태)\n",
    "data = normalization_x_data\n",
    "\n",
    "# 시퀀스 데이터 생성\n",
    "sequence_length = 3  # 시퀀스 길이 설정\n",
    "generator = TimeseriesGenerator(data, data, length=sequence_length, batch_size=1)\n",
    "\n",
    "# 데이터 분할\n",
    "X, y = [], []\n",
    "for i in range(len(generator)):\n",
    "    x, y_ = generator[i]\n",
    "    X.append(x[0])\n",
    "    y.append(y_[0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 훈련, 검증, 테스트 세트 분할\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train Set: {X_train.shape}, Validation Set: {X_val.shape}, Test Set: {X_test.shape}\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a386a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n##before\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\nimport numpy as np\\n\\n# 데이터셋 생성 함수\\ndef create_dataset(numbers, n_steps):\\n    X, y = [], []\\n    for i in range(len(numbers)):\\n        end_ix = i + n_steps\\n        if end_ix > len(numbers)-1:\\n            break\\n        seq_x, seq_y = numbers[i:end_ix], numbers[end_ix] # end_ix는 포함되지 않는다.\\n        X.append(seq_x)\\n        y.append(seq_y)\\n    return np.array(X), np.array(y)\\n\\n# 숫자 시퀀스 정의\\nnumbers = normalization_x_data\\nn_steps = 2 # 학습하고싶은 숫자묶음의 크기(발걸음)을 설정합니다.\\n\\n# 데이터셋 생성\\nX, y = create_dataset(numbers, n_steps) #데이터셋 생성 함수 활성화\\n\\nprint(X)\\nprint(y)\\n'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "##before\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_dataset(numbers, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(numbers)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(numbers)-1:\n",
    "            break\n",
    "        seq_x, seq_y = numbers[i:end_ix], numbers[end_ix] # end_ix는 포함되지 않는다.\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 숫자 시퀀스 정의\n",
    "numbers = normalization_x_data\n",
    "n_steps = 2 # 학습하고싶은 숫자묶음의 크기(발걸음)을 설정합니다.\n",
    "\n",
    "# 데이터셋 생성\n",
    "X, y = create_dataset(numbers, n_steps) #데이터셋 생성 함수 활성화\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "878ce30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 7ms/step - loss: 8.8993 - val_loss: 9.3072\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.6602 - val_loss: 6.4163\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.5923 - val_loss: 4.2657\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.2283 - val_loss: 4.3800\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0154 - val_loss: 4.0939\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.8159 - val_loss: 3.9086\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.6699 - val_loss: 3.8706\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.5194 - val_loss: 3.5707\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.3828 - val_loss: 3.5199\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.2884 - val_loss: 3.4306\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.1907 - val_loss: 3.4233\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.1082 - val_loss: 3.2629\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.0413 - val_loss: 3.2467\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.9931 - val_loss: 3.1176\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.9549 - val_loss: 3.2423\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.9323 - val_loss: 3.2048\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8958 - val_loss: 3.0687\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8888 - val_loss: 3.1537\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8733 - val_loss: 3.1437\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8776 - val_loss: 3.1250\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8659 - val_loss: 3.1006\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8688 - val_loss: 3.2319\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8760 - val_loss: 3.1771\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8651 - val_loss: 3.1420\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8566 - val_loss: 3.1117\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8662 - val_loss: 3.1287\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8640 - val_loss: 3.1044\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8768 - val_loss: 3.1252\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8943 - val_loss: 3.2461\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8716 - val_loss: 3.1700\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8628 - val_loss: 3.1442\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8556 - val_loss: 3.1521\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8485 - val_loss: 3.1422\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8490 - val_loss: 3.1361\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8485 - val_loss: 3.1055\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8526 - val_loss: 3.1619\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8589 - val_loss: 3.2219\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8664 - val_loss: 3.1032\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8544 - val_loss: 3.1307\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8704 - val_loss: 3.0890\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8599 - val_loss: 3.1473\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8428 - val_loss: 3.1363\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8521 - val_loss: 3.1771\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8532 - val_loss: 3.1421\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8484 - val_loss: 3.1465\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8440 - val_loss: 3.2010\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8510 - val_loss: 3.1573\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8544 - val_loss: 3.1299\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8541 - val_loss: 3.1393\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8520 - val_loss: 3.1993\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8539 - val_loss: 3.1717\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8507 - val_loss: 3.1602\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8551 - val_loss: 3.2444\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8580 - val_loss: 3.1386\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8423 - val_loss: 3.2068\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8557 - val_loss: 3.2430\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8551 - val_loss: 3.1881\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8477 - val_loss: 3.1161\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8554 - val_loss: 3.2262\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8580 - val_loss: 3.0967\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8723 - val_loss: 3.1898\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8519 - val_loss: 3.2064\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8540 - val_loss: 3.1363\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8410 - val_loss: 3.1987\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8456 - val_loss: 3.1602\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8380 - val_loss: 3.2137\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8466 - val_loss: 3.2253\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8449 - val_loss: 3.1991\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8432 - val_loss: 3.1926\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8393 - val_loss: 3.2036\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8429 - val_loss: 3.1373\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8519 - val_loss: 3.2379\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8334 - val_loss: 3.1469\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8408 - val_loss: 3.2221\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8414 - val_loss: 3.2029\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8358 - val_loss: 3.2020\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8437 - val_loss: 3.1303\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8490 - val_loss: 3.2370\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8489 - val_loss: 3.1174\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8459 - val_loss: 3.2323\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8460 - val_loss: 3.2484\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8326 - val_loss: 3.1862\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8405 - val_loss: 3.2092\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8339 - val_loss: 3.1648\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8413 - val_loss: 3.3077\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8519 - val_loss: 3.1591\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8410 - val_loss: 3.2016\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8366 - val_loss: 3.2167\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8461 - val_loss: 3.2384\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8320 - val_loss: 3.1531\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8493 - val_loss: 3.2110\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8511 - val_loss: 3.2688\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8332 - val_loss: 3.1847\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8365 - val_loss: 3.1785\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8337 - val_loss: 3.2432\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8526 - val_loss: 3.2250\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8442 - val_loss: 3.2637\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8373 - val_loss: 3.2086\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8336 - val_loss: 3.2438\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8374 - val_loss: 3.2029\n",
      "5/5 [==============================] - 0s 816us/step - loss: 2.7939\n",
      "Test Loss: 2.793883800506592\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(sequence_length, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
    "\n",
    "# 모델 성능 평가\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "70cdfc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n##before\\n# LSTM 모델 정의\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\nfrom keras.callbacks import EarlyStopping\\n\\nmodel = Sequential()\\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, 1)))\\nmodel.add(Dense(1))\\nmodel.compile(optimizer='adam', loss='mse')\\n\\n# 모델 훈련을 위해 X를 재구성\\nX = X.reshape((X.shape[0], X.shape[1], 1))\\n\\n# 조기 종료 콜백 설정\\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\\n\\n# 모델 훈련 (내가 생각했을떄 이것은 같은 상황의 반복이므로 에포크를 높게할수록 잘적중될것이다. 그러나 학습하더라도 램덤떄문에 성능개선에 한계가 있다)\\nmodel.fit(X, y, epochs=20, verbose=1, callbacks=[early_stopping])\\n\""
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "##before\n",
    "# LSTM 모델 정의\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 훈련을 위해 X를 재구성\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# 조기 종료 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련 (내가 생각했을떄 이것은 같은 상황의 반복이므로 에포크를 높게할수록 잘적중될것이다. 그러나 학습하더라도 램덤떄문에 성능개선에 한계가 있다)\n",
    "model.fit(X, y, epochs=20, verbose=1, callbacks=[early_stopping])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "12cfaf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization_input = [5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "def assign_numbers_based_on_range(numbers, ranges):\n",
    "    new_list = []\n",
    "    for number in numbers:\n",
    "        assigned = False\n",
    "        for range_start, range_end, assigned_number in ranges:\n",
    "            if range_start <= number <= range_end:\n",
    "                new_list.append(assigned_number)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            new_list.append(ranges[-1][2])  # 범위에 속하지 않는 경우 마지막 숫자를 할당(range의 마지막 요소 세번째 요소)\n",
    "    return new_list\n",
    "\n",
    "# 사용 예시\n",
    "numbers = [209, 109, 68]\n",
    "ranges = [(1, 51, 1), (51, 101, 2), (101, 151, 3), (151, 201, 4),(201, 251, 5),(251, 301, 6)] # 각 세번째 요소는 할당할 숫자를 의미합니다.\n",
    "normalization_input = assign_numbers_based_on_range(numbers, ranges)\n",
    "print(\"normalization_input =\", normalization_input)\n",
    "\n",
    "## 여기까지가 새로들어온 input값 구간으로 정규화시켜주는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9ccde808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "Predicted Value: 2.633702278137207\n",
      "Predicted Value with Error Margin: 2.533702278137207 to 2.733702278137207\n"
     ]
    }
   ],
   "source": [
    "new_sequence = np.array([5, 3, 2])  # 새 시퀀스 값들\n",
    "new_sequence = new_sequence.reshape((1, sequence_length, 1))  # 모델에 맞게 형태 변환\n",
    "\n",
    "# 예측 수행\n",
    "predicted_value = model.predict(new_sequence)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "\n",
    "# 예측의 불확실성 추정 (간단한 예시)\n",
    "# 이 부분은 실제 확률을 나타내는 것이 아니라, 단순히 오차를 기반으로 한 추정입니다.\n",
    "error_margin = 0.1  # 가정된 오차 범위\n",
    "lower_bound = predicted_value[0][0] - error_margin\n",
    "upper_bound = predicted_value[0][0] + error_margin\n",
    "print(f\"Predicted Value with Error Margin: {lower_bound} to {upper_bound}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f163dbf",
   "metadata": {},
   "source": [
    "# 내가 만든 모델 성능 알수있도록 해주는 방법 만들기(확률을 나타내든 정확도를 나타내든)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
